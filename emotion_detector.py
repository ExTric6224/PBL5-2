# emotion_detector.py
import cv2
import numpy as np
import argparse
import os
import time
import threading
from datetime import datetime
import socket
import struct

# --- Nh·∫≠p c√°c l·ªõp Analyzer ---
from face_analyzer import FaceAnalyzer # Gi·∫£ s·ª≠ b·∫°n c√≥ file n√†y
from emotion_history_item import EmotionHistoryItem # Gi·∫£ s·ª≠ b·∫°n c√≥ file n√†y
from voice_analyzer import VoiceAnalyzer # Gi·∫£ s·ª≠ b·∫°n c√≥ file n√†y

# db
from db_utils import load_emotion_history_from_db # Gi·∫£ s·ª≠ b·∫°n c√≥ file n√†y

# --- Th∆∞ vi·ªán √¢m thanh (cho x·ª≠ l√Ω local) ---
SOUND_DEVICE_AVAILABLE = False # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t d·ª±a tr√™n VoiceAnalyzer n·∫øu d√πng mic local
try:
    # V√≠ d·ª•, n·∫øu VoiceAnalyzer ki·ªÉm tra sounddevice
    # Ho·∫∑c b·∫°n c√≥ th·ªÉ b·ªè qua n·∫øu VoiceAnalyzer kh√¥ng d√πng mic local n·ªØa
    if hasattr(VoiceAnalyzer, 'check_sound_device_availability'): # Gi·∫£ s·ª≠ c√≥ h√†m n√†y
        SOUND_DEVICE_AVAILABLE = VoiceAnalyzer.check_sound_device_availability()
    else: # Ho·∫∑c n·∫øu VoiceAnalyzer lu√¥n d√πng mic local v√† kh√¥ng c√≥ check
        import sounddevice as sd # Th·ª≠ import ƒë·ªÉ xem c√≥ l·ªói kh√¥ng
        SOUND_DEVICE_AVAILABLE = True
except ImportError:
    print("C·∫£nh b√°o: Th∆∞ vi·ªán sounddevice kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y cho audio local.")
    SOUND_DEVICE_AVAILABLE = False
except Exception as e:
    print(f"L·ªói khi ki·ªÉm tra sounddevice: {e}")
    SOUND_DEVICE_AVAILABLE = False


class EmotionDetector:
    def __init__(self, face_analyzer, voice_analyzer, cascade_path='models/haarcascade_frontalface_default.xml',
                 enable_analysis_face=True, enable_analysis_voice=True,
                 camera_host='0.0.0.0', camera_port=9999,
                 # C√°c tham s·ªë cho audio server (n·∫øu mu·ªën nh·∫≠n audio t·ª´ m·∫°ng)
                 # audio_host='0.0.0.0', audio_port=9998,
                 # receive_audio_from_network=False 
                 ):
        if not isinstance(face_analyzer, FaceAnalyzer):
            raise TypeError("'face_analyzer' ph·∫£i l√† m·ªôt instance c·ªßa FaceAnalyzer.")
        self.face_analyzer = face_analyzer
        self.voice_analyzer = voice_analyzer
        self.face_cascade = self._load_cascade(cascade_path)

        if self.face_cascade is None:
            # ƒê√£ c√≥ exit trong __main__, c√≥ th·ªÉ ch·ªâ c·∫ßn print ·ªü ƒë√¢y n·∫øu mu·ªën class linh ho·∫°t h∆°n
            print(f"L·ªñI NGHI√äM TR·ªåNG: Kh√¥ng th·ªÉ t·∫£i Haar cascade t·ª´: {cascade_path}")
            raise RuntimeError(f"Kh√¥ng th·ªÉ t·∫£i Haar cascade t·ª´: {cascade_path}")


        self.latest_frame = None
        self.last_face_emotion = "N/A"
        self.last_face_emotion_probabilities = None
        
        self.last_voice_emotion = "N/A (Local Mic)" # Thay ƒë·ªïi n·∫øu nh·∫≠n t·ª´ m·∫°ng
        self.last_voice_probabilities = None
        self.frame_lock = threading.Lock()
        self.emotion_lock = threading.Lock()

        self.stop_event = threading.Event()
        self.face_thread = None
        self.voice_thread = None
        
        self.enable_analysis_face = enable_analysis_face
        self.enable_analysis_voice = enable_analysis_voice
        self.emotion_history = load_emotion_history_from_db("emotion_log.db")
        self.can_send_to_UI = True

        # --- Network Server cho Camera ---
        self.camera_host = camera_host
        self.camera_port = camera_port
        self.camera_server_socket = None
        self.camera_client_conn = None
        self.camera_source_initialized = False

        # --- C·∫•u h√¨nh cho Audio (hi·ªán t·∫°i l√† local, c√≥ th·ªÉ m·ªü r·ªông) ---
        # self.receive_audio_from_network = receive_audio_from_network
        # self.audio_host = audio_host
        # self.audio_port = audio_port
        # self.audio_server_socket = None
        # self.audio_client_conn = None
        # self.audio_source_initialized = False # C·∫ßn n·∫øu nh·∫≠n audio t·ª´ m·∫°ng


    def _load_cascade(self, cascade_path):
        if not os.path.exists(cascade_path):
            print(f"L·ªói: File cascade kh√¥ng t·ªìn t·∫°i t·∫°i {cascade_path}")
            return None
        try:
            cascade = cv2.CascadeClassifier(cascade_path)
            if cascade.empty():
                print(f"L·ªói: File cascade t·∫°i {cascade_path} tr·ªëng ho·∫∑c kh√¥ng h·ª£p l·ªá.")
                return None
            print(f"Cascade ƒë√£ t·∫£i th√†nh c√¥ng t·ª´ {cascade_path}")
            return cascade
        except Exception as e:
            print(f"L·ªói khi t·∫£i cascade t·ª´ {cascade_path}: {e}")
            return None

    def _init_camera_server(self):
        """Kh·ªüi t·∫°o server socket ƒë·ªÉ nh·∫≠n d·ªØ li·ªáu h√¨nh ·∫£nh t·ª´ Raspberry Pi."""
        try:
            self.camera_server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.camera_server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            self.camera_server_socket.bind((self.camera_host, self.camera_port))
            self.camera_server_socket.listen(1)
            print(f"üì∑ Camera Server: ƒêang l·∫Øng nghe tr√™n {self.camera_host}:{self.camera_port} ch·ªù Raspberry Pi k·∫øt n·ªëi...")
            self.camera_client_conn, client_addr = self.camera_server_socket.accept()
            print(f"‚úÖ Camera Server: Raspberry Pi t·ª´ {client_addr} ƒë√£ k·∫øt n·ªëi.")
            self.camera_source_initialized = True
            return True
        except Exception as e:
            print(f"‚ùå Camera Server: L·ªói kh·ªüi t·∫°o socket: {e}")
            self.camera_source_initialized = False
            if self.camera_server_socket:
                self.camera_server_socket.close()
            return False

    def _face_processing_loop(self):
        print("üôÇ Lu·ªìng x·ª≠ l√Ω khu√¥n m·∫∑t (nh·∫≠n t·ª´ m·∫°ng): B·∫Øt ƒë·∫ßu.")
        if not self.camera_source_initialized or not self.camera_client_conn:
            print("‚ùå Lu·ªìng khu√¥n m·∫∑t: Ngu·ªìn camera (socket) ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o ho·∫∑c ch∆∞a c√≥ k·∫øt n·ªëi.")
            return

        active_connection = True
        while not self.stop_event.is_set() and active_connection:
            if not self.can_send_to_UI:
                time.sleep(1) # Gi·∫£m t·∫£i CPU n·∫øu UI kh√¥ng s·∫µn s√†ng
                continue
            
            try:
                # 1. ƒê·ªçc 4 byte ƒë·∫ßu ti√™n ƒë·ªÉ l·∫•y ƒë·ªô d√†i d·ªØ li·ªáu ·∫£nh
                packed_image_len = self.camera_client_conn.recv(4)
                if not packed_image_len:
                    print("‚ö†Ô∏è Lu·ªìng khu√¥n m·∫∑t: Client (Pi) ƒë√£ ng·∫Øt k·∫øt n·ªëi (kh√¥ng g·ª≠i ƒë·ªô d√†i).")
                    active_connection = False
                    break
                
                image_len = struct.unpack('>L', packed_image_len)[0]
                if image_len == 0: # C√≥ th·ªÉ l√† t√≠n hi·ªáu ƒë·∫∑c bi·ªát ho·∫∑c l·ªói
                    print("‚ö†Ô∏è Lu·ªìng khu√¥n m·∫∑t: Nh·∫≠n ƒë∆∞·ª£c ƒë·ªô d√†i ·∫£nh b·∫±ng 0.")
                    time.sleep(0.1)
                    continue

                # 2. ƒê·ªçc d·ªØ li·ªáu ·∫£nh (JPEG bytes)
                image_data = b''
                while len(image_data) < image_len:
                    packet = self.camera_client_conn.recv(image_len - len(image_data))
                    if not packet:
                        print("‚ö†Ô∏è Lu·ªìng khu√¥n m·∫∑t: Client (Pi) ng·∫Øt k·∫øt n·ªëi khi ƒëang g·ª≠i d·ªØ li·ªáu ·∫£nh.")
                        active_connection = False
                        break
                    image_data += packet
                
                if not active_connection: break # Tho√°t n·∫øu k·∫øt n·ªëi ƒë√£ m·∫•t

                # 3. Gi·∫£i m√£ ·∫£nh JPEG th√†nh frame OpenCV
                image_array = np.frombuffer(image_data, dtype=np.uint8)
                frame = cv2.imdecode(image_array, cv2.IMREAD_COLOR)

                if frame is None:
                    print(" L·ªói Lu·ªìng khu√¥n m·∫∑t: Kh√¥ng th·ªÉ gi·∫£i m√£ frame nh·∫≠n ƒë∆∞·ª£c t·ª´ Pi.")
                    time.sleep(0.1)
                    continue

                # --- Ph·∫ßn x·ª≠ l√Ω c·∫£m x√∫c gi·ªØ nguy√™n ---
                if self.enable_analysis_face:   
                    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                    # (Gi·ªØ l·∫°i x, y, w, h ·ªü scope r·ªông h∆°n n·∫øu c·∫ßn cho EmotionHistoryItem khi kh√¥ng c√≥ face)
                    detected_faces_in_frame = self.face_cascade.detectMultiScale(gray_frame, 1.1, 5, minSize=(40, 40))
                    processed_frame = frame.copy() # Lu√¥n t·∫°o processed_frame
                    current_face_emotion_in_frame = "N/A"
                    current_emotion_prob = 0.0
                    face_coords = "N/A" # Kh·ªüi t·∫°o t·ªça ƒë·ªô
                    predicted_distribution = None # Kh·ªüi t·∫°o ph√¢n ph·ªëi

                    for (x, y, w, h) in detected_faces_in_frame:
                        cv2.rectangle(processed_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
                        roi_gray = gray_frame[y:y + h, x:x + w]
                        predicted = self.face_analyzer.analyzeFace(roi_gray)
                        if predicted:
                            current_face_emotion_in_frame, current_emotion_prob = max(predicted.items(), key=lambda item: item[1])
                            face_coords = f"{x}x{y}" # C·∫≠p nh·∫≠t t·ªça ƒë·ªô
                            predicted_distribution = predicted # L∆∞u ph√¢n ph·ªëi
                            # Hi·ªÉn th·ªã c·∫£m x√∫c l√™n frame (v√≠ d·ª•)
                            cv2.putText(processed_frame, f"{current_face_emotion_in_frame} ({current_emotion_prob:.2f})",
                                        (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                            break # Ch·ªâ x·ª≠ l√Ω khu√¥n m·∫∑t ƒë·∫ßu ti√™n

                    with self.frame_lock:
                        self.latest_frame = processed_frame

                    # Ch·ªâ c·∫≠p nh·∫≠t v√† ghi l·ªãch s·ª≠ n·∫øu c√≥ s·ª± thay ƒë·ªïi ho·∫∑c l√† N/A
                    # Ho·∫∑c n·∫øu b·∫°n mu·ªën ghi l·∫°i m·ªçi frame c√≥ c·∫£m x√∫c (ngay c·∫£ khi gi·ªëng frame tr∆∞·ªõc)
                    # if current_face_emotion_in_frame != "N/A": # ƒêi·ªÅu ki·ªán n√†y c√≥ th·ªÉ h∆°i ch·∫∑t
                    with self.emotion_lock:
                        if self.last_face_emotion != current_face_emotion_in_frame or current_face_emotion_in_frame == "N/A":
                            if current_face_emotion_in_frame != "N/A" and predicted_distribution:
                                emotion_item = EmotionHistoryItem(
                                    timestamp=datetime.now(),
                                    face_location=face_coords,
                                    duration=None,
                                    result=current_face_emotion_in_frame,
                                    source="NetworkCamera", # Ngu·ªìn t·ª´ Camera Pi g·ª≠i qua
                                    emotion_distribution=predicted_distribution
                                )
                                self.emotion_history.append(emotion_item)
                        
                        self.last_face_emotion = current_face_emotion_in_frame
                        self.last_face_emotion_probabilities = current_emotion_prob
                
                else: # self.enable_analysis_face is False
                    with self.frame_lock:
                        self.latest_frame = frame.copy() # G·ª≠i frame g·ªëc n·∫øu kh√¥ng ph√¢n t√≠ch
                    with self.emotion_lock:
                        self.last_face_emotion = "N/A"
                        self.last_face_emotion_probabilities = 0.0
            
            except socket.error as se:
                print(f"‚ùå Lu·ªìng khu√¥n m·∫∑t: L·ªói socket khi nh·∫≠n d·ªØ li·ªáu: {se}")
                active_connection = False # D·ª´ng v√≤ng l·∫∑p n·∫øu c√≥ l·ªói socket nghi√™m tr·ªçng
            except struct.error as ste:
                print(f"‚ùå Lu·ªìng khu√¥n m·∫∑t: L·ªói gi·∫£i n√©n d·ªØ li·ªáu (struct): {ste}")
                # C√≥ th·ªÉ client g·ª≠i d·ªØ li·ªáu sai ƒë·ªãnh d·∫°ng, c√¢n nh·∫Øc ƒë√≥ng k·∫øt n·ªëi
                active_connection = False
            except Exception as e:
                print(f"‚ùå Lu·ªìng khu√¥n m·∫∑t: L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}")
                import traceback
                traceback.print_exc()
                time.sleep(0.5) # ƒê·ª£i m·ªôt ch√∫t n·∫øu c√≥ l·ªói l·∫°

        print("üôÇ Lu·ªìng x·ª≠ l√Ω khu√¥n m·∫∑t (nh·∫≠n t·ª´ m·∫°ng): ƒê√£ d·ª´ng.")
        # D·ªçn d·∫πp socket ·ªü h√†m stop() ho·∫∑c cleanup()
        if self.camera_client_conn:
            try:
                self.camera_client_conn.close()
            except: pass # B·ªè qua l·ªói n·∫øu ƒë√£ ƒë√≥ng
            self.camera_client_conn = None
        # Kh√¥ng ƒë√≥ng server_socket ·ªü ƒë√¢y ƒë·ªÉ c√≥ th·ªÉ ch·∫•p nh·∫≠n k·∫øt n·ªëi m·ªõi n·∫øu c·∫ßn,
        # tr·ª´ khi ch∆∞∆°ng tr√¨nh d·ª´ng h·∫≥n. Vi·ªác ƒë√≥ng server_socket n√™n ·ªü stop() ho·∫∑c cleanup().

    def get_latest_data(self): # Gi·ªØ nguy√™n
        frame = None
        face_emo = "N/A"
        face_emo_prob = 0.0
        voice_emo = self.last_voice_emotion # L·∫•y t·ª´ self
        voice_emo_pros = self.last_voice_probabilities # L·∫•y t·ª´ self

        with self.frame_lock:
            if self.latest_frame is not None:
                frame = self.latest_frame.copy()

        with self.emotion_lock:
            face_emo = self.last_face_emotion
            face_emo_prob = self.last_face_emotion_probabilities if self.last_face_emotion_probabilities is not None else 0.0
        
        if not isinstance(face_emo_prob, float): # ƒê·∫£m b·∫£o l√† float
            try: face_emo_prob = float(face_emo_prob)
            except: face_emo_prob = 0.0

        return frame, face_emo, face_emo_prob, voice_emo, voice_emo_pros

    def _voice_processing_loop(self):
        """
        V√≤ng l·∫∑p x·ª≠ l√Ω √¢m thanh. Hi·ªán t·∫°i d√πng microphone local.
        ƒê·ªÉ nh·∫≠n t·ª´ m·∫°ng:
        1. Trong __init__, kh·ªüi t·∫°o self.audio_server_socket, self.audio_client_conn t∆∞∆°ng t·ª± camera.
        2. Trong start(), g·ªçi m·ªôt h√†m _init_audio_server() ƒë·ªÉ bind, listen, accept.
        3. Trong v√≤ng l·∫∑p n√†y, thay v√¨ self.voice_analyzer.record_audio(),
           h√£y conn.recv() d·ªØ li·ªáu audio (ƒë·ªô d√†i + m·∫£ng float32) t·ª´ Pi.
        4. Chuy·ªÉn ƒë·ªïi bytes nh·∫≠n ƒë∆∞·ª£c th√†nh m·∫£ng NumPy float32.
        5. G·ªçi self.voice_analyzer.predict_emotion(audio_array_from_network).
        """
        print("üé§ Lu·ªìng √¢m thanh (local mic): B·∫Øt ƒë·∫ßu.")
        if not SOUND_DEVICE_AVAILABLE and self.enable_analysis_voice:
            print("C·∫£nh b√°o: Sounddevice kh√¥ng s·∫µn s√†ng, kh√¥ng th·ªÉ x·ª≠ l√Ω √¢m thanh local.")
            # V√¥ hi·ªáu h√≥a ph√¢n t√≠ch gi·ªçng n√≥i n·∫øu kh√¥ng c√≥ thi·∫øt b·ªã
            # Ho·∫∑c n·∫øu sau n√†y b·∫°n chuy·ªÉn sang nh·∫≠n audio t·ª´ m·∫°ng, ph·∫ßn n√†y s·∫Ω kh√°c.
            with self.emotion_lock:
                self.last_voice_emotion = "N/A (No Mic)"
                self.last_voice_probabilities = None
            self.enable_analysis_voice = False # T·∫°m v√¥ hi·ªáu h√≥a
            # return # Tho√°t n·∫øu kh√¥ng c√≥ mic v√† ƒëang b·∫≠t ph√¢n t√≠ch

        while not self.stop_event.is_set():
            if not self.enable_analysis_voice:
                # N·∫øu kh√¥ng ph√¢n t√≠ch gi·ªçng n√≥i, c·∫≠p nh·∫≠t tr·∫°ng th√°i N/A v√† ng·ªß
                if self.voice_analyzer and hasattr(self.voice_analyzer, 'emotion_labels'):
                     probabilities = {label: 0.0 for label in self.voice_analyzer.emotion_labels}
                else: # Fallback n·∫øu voice_analyzer kh√¥ng c√≥ emotion_labels
                    probabilities = {"neutral":0.0, "happy":0.0, "sad":0.0, "angry":0.0, "fearful":0.0, "disgusted":0.0, "surprised":0.0} # V√≠ d·ª•
                with self.emotion_lock:
                    self.last_voice_emotion = "N/A"
                    self.last_voice_probabilities = probabilities
                time.sleep(1)
                continue

            # === Logic x·ª≠ l√Ω √¢m thanh local (gi·ªØ nguy√™n t·ª´ code g·ªëc c·ªßa b·∫°n) ===
            try:
                # Gi·∫£ s·ª≠ voice_analyzer.record_audio() tr·∫£ v·ªÅ m·∫£ng numpy
                # v√† voice_analyzer.predict_emotion() nh·∫≠n m·∫£ng numpy
                if not self.voice_analyzer: # Ki·ªÉm tra xem voice_analyzer c√≥ t·ªìn t·∫°i kh√¥ng
                    print("L·ªói: VoiceAnalyzer ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o.")
                    time.sleep(1)
                    continue

                audio_array = self.voice_analyzer.record_audio() # Ghi √¢m local
                if audio_array is None: # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p record_audio tr·∫£ v·ªÅ None (v√≠ d·ª•: l·ªói)
                    print("Kh√¥ng nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu audio t·ª´ record_audio.")
                    time.sleep(1)
                    continue

                probabilities = self.voice_analyzer.predict_emotion(audio_array)

                if probabilities is None or "error" in probabilities: # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p predict_emotion l·ªói
                    print("L·ªói khi d·ª± ƒëo√°n c·∫£m x√∫c gi·ªçng n√≥i ho·∫∑c kh√¥ng c√≥ k·∫øt qu·∫£.")
                    # ƒê·∫∑t gi√° tr·ªã m·∫∑c ƒë·ªãnh an to√†n
                    if hasattr(self.voice_analyzer, 'emotion_labels'):
                        safe_probabilities = {label: 0.0 for label in self.voice_analyzer.emotion_labels}
                    else:
                        safe_probabilities = {"neutral":1.0} # V√≠ d·ª•
                    with self.emotion_lock:
                        self.last_voice_emotion = "Error/None"
                        self.last_voice_probabilities = safe_probabilities
                    time.sleep(1)
                    continue
                
                with self.emotion_lock:
                    self.last_voice_emotion = max(probabilities, key=probabilities.get)
                    self.last_voice_probabilities = probabilities
                    
                    duration_ms = 0
                    if hasattr(self.voice_analyzer, 'duration_sec'):
                        duration_ms = int(self.voice_analyzer.duration_sec * 1000)

                    emotion_item = EmotionHistoryItem(
                        timestamp=datetime.now(),
                        face_location=None,
                        duration=duration_ms,
                        result=self.last_voice_emotion,
                        source="Microphone (Local)",
                        emotion_distribution=probabilities
                    )
                    self.emotion_history.append(emotion_item)

            except AttributeError as ae: # B·∫Øt l·ªói n·∫øu voice_analyzer thi·∫øu ph∆∞∆°ng th·ª©c
                print(f"L·ªói thu·ªôc t√≠nh trong VoiceAnalyzer: {ae}. ƒê·∫£m b·∫£o VoiceAnalyzer ƒë∆∞·ª£c tri·ªÉn khai ƒë√∫ng.")
                # T·∫°m d·ª´ng ph√¢n t√≠ch gi·ªçng n√≥i n·∫øu c√≥ l·ªói nghi√™m tr·ªçng v·ªõi analyzer
                self.enable_analysis_voice = False 
                with self.emotion_lock:
                    self.last_voice_emotion = "Analyzer Error"
                    self.last_voice_probabilities = None
            except Exception as e:
                print(f"L·ªói trong lu·ªìng x·ª≠ l√Ω √¢m thanh: {e}")
                # C√≥ th·ªÉ ƒë·∫∑t last_voice_emotion v·ªÅ tr·∫°ng th√°i l·ªói ·ªü ƒë√¢y
                with self.emotion_lock:
                    self.last_voice_emotion = "Error"
                    self.last_voice_probabilities = None # Ho·∫∑c m·ªôt dict x√°c su·∫•t l·ªói
            
            # Th√™m sleep nh·ªè ƒë·ªÉ tr√°nh v√≤ng l·∫∑p qu√° nhanh n·∫øu record/predict nhanh
            time.sleep(0.1) # Ho·∫∑c d·ª±a tr√™n duration c·ªßa voice_analyzer
        print("üé§ Lu·ªìng √¢m thanh (local mic): ƒê√£ d·ª´ng.")


    def start(self):
        if self.face_thread or self.voice_thread:
            print("Detector ƒë√£ ch·∫°y r·ªìi.")
            return

        # Kh·ªüi t·∫°o server camera
        if not self._init_camera_server():
            # Kh√¥ng raise RuntimeError ·ªü ƒë√¢y n·ªØa ƒë·ªÉ cho ph√©p audio (n·∫øu c√≥) v·∫´n ch·∫°y
            # Ho·∫∑c n·∫øu camera l√† b·∫Øt bu·ªôc th√¨ raise
            print("L·ªñI NGHI√äM TR·ªåNG: Kh√¥ng th·ªÉ kh·ªüi t·∫°o Camera Server. Lu·ªìng khu√¥n m·∫∑t s·∫Ω kh√¥ng ho·∫°t ƒë·ªông.")
            # self.camera_source_initialized s·∫Ω l√† False
        else:
            print("Camera server ƒë√£ s·∫µn s√†ng.")


        # Kh·ªüi t·∫°o audio (hi·ªán t·∫°i l√† local, n·∫øu l√† server th√¨ t∆∞∆°ng t·ª± camera)
        # if self.receive_audio_from_network:
        #     if not self._init_audio_server(): # C·∫ßn t·∫°o h√†m n√†y
        #         print("L·ªñI NGHI√äM TR·ªåNG: Kh√¥ng th·ªÉ kh·ªüi t·∫°o Audio Server.")
        # else:
        #     # Ki·ªÉm tra mic local n·∫øu kh√¥ng nh·∫≠n t·ª´ m·∫°ng v√† voice analysis ƒë∆∞·ª£c b·∫≠t
        if self.enable_analysis_voice and not SOUND_DEVICE_AVAILABLE:
             print("C·∫£nh b√°o: Ph√¢n t√≠ch gi·ªçng n√≥i ƒë∆∞·ª£c b·∫≠t nh∆∞ng kh√¥ng t√¨m th·∫•y thi·∫øt b·ªã sounddevice local.")
             # C√≥ th·ªÉ quy·∫øt ƒë·ªãnh d·ª´ng ·ªü ƒë√¢y ho·∫∑c ch·ªâ v√¥ hi·ªáu h√≥a voice_thread


        self.stop_event.clear()
        print("B·∫Øt ƒë·∫ßu c√°c lu·ªìng x·ª≠ l√Ω n·ªÅn...")
        
        if self.camera_source_initialized: # Ch·ªâ start face_thread n·∫øu server camera OK
            self.face_thread = threading.Thread(target=self._face_processing_loop, name="FaceProcessingThread", daemon=True)
            self.face_thread.start()
        else:
            print("Lu·ªìng khu√¥n m·∫∑t kh√¥ng ƒë∆∞·ª£c kh·ªüi ƒë·ªông do Camera Server l·ªói.")

        # Lu·ªìng voice v·∫´n c√≥ th·ªÉ ch·∫°y v·ªõi mic local (ho·∫∑c server audio n·∫øu b·∫°n tri·ªÉn khai)
        if self.enable_analysis_voice: # Ch·ªâ start n·∫øu ƒë∆∞·ª£c enable
            self.voice_thread = threading.Thread(target=self._voice_processing_loop, name="VoiceProcessingThread", daemon=True)
            self.voice_thread.start()
        else:
            print("Lu·ªìng gi·ªçng n√≥i kh√¥ng ƒë∆∞·ª£c kh·ªüi ƒë·ªông (ƒë√£ b·ªã v√¥ hi·ªáu h√≥a).")


    def stop(self):
        if self.stop_event.is_set(): return
        print("B·∫Øt ƒë·∫ßu qu√° tr√¨nh d·ª´ng EmotionDetector...")
        self.stop_event.set()

        threads_to_join = []
        if self.face_thread and self.face_thread.is_alive():
            threads_to_join.append(self.face_thread)
        if self.voice_thread and self.voice_thread.is_alive():
            threads_to_join.append(self.voice_thread)

        for t in threads_to_join:
            print(f"ƒêang ƒë·ª£i lu·ªìng {t.name} d·ª´ng...")
            t.join(timeout=3.0) # TƒÉng timeout m·ªôt ch√∫t
            if t.is_alive(): print(f"C·∫£nh b√°o: Lu·ªìng {t.name} kh√¥ng d·ª´ng k·ªãp th·ªùi.")
        
        print("C√°c lu·ªìng x·ª≠ l√Ω n·ªÅn ƒë√£ d·ª´ng (ho·∫∑c ƒë√£ ƒë∆∞·ª£c y√™u c·∫ßu d·ª´ng).")
        self.cleanup() # G·ªçi cleanup sau khi c√°c lu·ªìng ƒë√£ join

    def cleanup(self):
        print("EmotionDetector: Th·ª±c hi·ªán cleanup cu·ªëi c√πng.")
        # ƒê√≥ng socket camera
        if self.camera_client_conn:
            try: self.camera_client_conn.close()
            except Exception as e: print(f"L·ªói khi ƒë√≥ng camera_client_conn: {e}")
            self.camera_client_conn = None
        if self.camera_server_socket:
            try: self.camera_server_socket.close()
            except Exception as e: print(f"L·ªói khi ƒë√≥ng camera_server_socket: {e}")
            self.camera_server_socket = None
        print("C√°c socket camera ƒë√£ ƒë∆∞·ª£c ƒë√≥ng (n·∫øu c√≥).")
        
        # T∆∞∆°ng t·ª± cho audio socket n·∫øu b·∫°n tri·ªÉn khai server audio
        # if self.audio_client_conn:
        #     try: self.audio_client_conn.close()
        #     except: pass
        # if self.audio_server_socket:
        #     try: self.audio_server_socket.close()
        #     except: pass
        # print("C√°c socket audio ƒë√£ ƒë∆∞·ª£c ƒë√≥ng (n·∫øu c√≥).")
        pass


# --- ƒêi·ªÉm kh·ªüi ch·∫°y ch√≠nh ---
if __name__ == "__main__": # S·ª≠a th√†nh "__main__"
    # --- Import UIController ·ªü ƒë√¢y ---
    # ƒê·∫£m b·∫£o UIController c·ªßa b·∫°n c√≥ th·ªÉ x·ª≠ l√Ω vi·ªác frame ƒë·∫øn tr·ªÖ ho·∫∑c kh√¥ng c√≥
    try:
        from ui_controller import EmotionGUI 
    except ImportError:
        print("L·ªñI: Kh√¥ng t√¨m th·∫•y ui_controller.py. Giao di·ªán s·∫Ω kh√¥ng ho·∫°t ƒë·ªông.")
        # C√≥ th·ªÉ tho√°t ·ªü ƒë√¢y ho·∫∑c ch·∫°y kh√¥ng c√≥ UI n·∫øu detector c√≥ th·ªÉ ho·∫°t ƒë·ªông ƒë·ªôc l·∫≠p
        EmotionGUI = None 


    ap = argparse.ArgumentParser(description="Ch·∫°y nh·∫≠n di·ªán c·∫£m x√∫c ƒëa lu·ªìng v·ªõi UI Controller (Server Mode).")
    ap.add_argument("--face_model", default="models/emotion_model.h5", help="ƒê∆∞·ªùng d·∫´n face model")
    ap.add_argument("--cascade", default="models/haarcascade_frontalface_default.xml", help="ƒê∆∞·ªùng d·∫´n Haar cascade")
    # Th√™m c√°c argument cho host/port n·∫øu mu·ªën t√πy ch·ªânh t·ª´ d√≤ng l·ªánh
    ap.add_argument("--camera_host", default="0.0.0.0", help="ƒê·ªãa ch·ªâ IP ƒë·ªÉ Camera Server l·∫Øng nghe")
    ap.add_argument("--camera_port", default=9999, type=int, help="C·ªïng ƒë·ªÉ Camera Server l·∫Øng nghe")
    # ap.add_argument("--receive_audio_network", action='store_true', help="Nh·∫≠n audio t·ª´ m·∫°ng thay v√¨ mic local")

    args = ap.parse_args()

    # --- Ki·ªÉm tra file ---
    if not os.path.exists(args.cascade):
        exit(f"L·ªói: Cascade kh√¥ng t√¨m th·∫•y t·∫°i ƒë∆∞·ªùng d·∫´n ƒë∆∞·ª£c cung c·∫•p: {args.cascade}. Vui l√≤ng ki·ªÉm tra l·∫°i.")
    if not os.path.exists(args.face_model):
        exit(f"L·ªói: Face model kh√¥ng t√¨m th·∫•y t·∫°i: {args.face_model}. Vui l√≤ng ki·ªÉm tra l·∫°i.")

    main_detector = None
    root = None # Khai b√°o ƒë·ªÉ c√≥ th·ªÉ truy c·∫≠p trong finally n·∫øu c·∫ßn

    try:
        face_analyzer_inst = FaceAnalyzer(model_path=args.face_model)
        voi_analyzer_inst = VoiceAnalyzer() # Gi·∫£ s·ª≠ VoiceAnalyzer c√≥ th·ªÉ kh·ªüi t·∫°o kh√¥ng tham s·ªë
                                            # ho·∫∑c b·∫°n c·∫ßn c·∫•u h√¨nh n√≥ cho mic local/network
        
        main_detector = EmotionDetector(
            face_analyzer=face_analyzer_inst,
            voice_analyzer=voi_analyzer_inst,
            cascade_path=args.cascade,
            camera_host=args.camera_host,
            camera_port=args.camera_port,
            # receive_audio_from_network=args.receive_audio_network # N·∫øu c√≥ arg n√†y
        )

        main_detector.start() # Kh·ªüi t·∫°o server v√† c√°c lu·ªìng
        print("EmotionDetector (Server Mode) ƒë√£ kh·ªüi ƒë·ªông. ƒêang ch·ªù client (Raspberry Pi) k·∫øt n·ªëi...")

        if EmotionGUI: # Ch·ªâ ch·∫°y UI n·∫øu import th√†nh c√¥ng
            from ttkbootstrap import Style
            try:
                style = Style(theme="superhero") 
                root = style.master
            except Exception as e: # B·∫Øt l·ªói n·∫øu ttkbootstrap c√≥ v·∫•n ƒë·ªÅ
                print(f"L·ªói kh·ªüi t·∫°o style/root cho ttkbootstrap: {e}")
                print("S·∫Ω th·ª≠ t·∫°o Tk root c∆° b·∫£n.")
                import tkinter as tk
                root = tk.Tk()
                root.title("Emotion Detector (Basic Fallback UI)")


            gui = EmotionGUI(root) # Truy·ªÅn root (master_window) v√†o EmotionGUI
            gui.detector = main_detector 

            def update_gui():
                # L·∫•y d·ªØ li·ªáu an to√†n, c√≥ th·ªÉ l√† None n·∫øu ch∆∞a c√≥ g√¨
                frame, face_emo, face_emo_pro, voice_emo, voice_emo_pros = main_detector.get_latest_data()
                
                # update_video_frame c·∫ßn c√≥ kh·∫£ nƒÉng x·ª≠ l√Ω frame=None
                gui.update_video_frame(
                    frame, 
                    face_emotion=face_emo,
                    emotion_probability=face_emo_pro,
                    voice_emotion=voice_emo,
                    voice_probabilities=voice_emo_pros
                )
                if root: # Ch·ªâ g·ªçi after n·∫øu root t·ªìn t·∫°i
                    root.after(50, update_gui) # TƒÉng delay m·ªôt ch√∫t cho server mode
            
            if root: # Ch·ªâ ch·∫°y mainloop n·∫øu root t·ªìn t·∫°i
                update_gui()
                root.mainloop()
            else:
                print("Kh√¥ng th·ªÉ kh·ªüi t·∫°o root UI. Ch·∫°y ·ªü ch·∫ø ƒë·ªô kh√¥ng UI.")
                # Gi·ªØ lu·ªìng ch√≠nh ch·∫°y ƒë·ªÉ c√°c lu·ªìng con (detector) ti·∫øp t·ª•c
                while not main_detector.stop_event.is_set():
                    try:
                        time.sleep(1)
                    except KeyboardInterrupt:
                        print("\nPh√°t hi·ªán Ctrl+C! ƒêang d·ª´ng detector...")
                        break # Tho√°t v√≤ng l·∫∑p n√†y, finally s·∫Ω ƒë∆∞·ª£c g·ªçi
        else:
            print("Ch·∫°y ·ªü ch·∫ø ƒë·ªô kh√¥ng UI (UIController kh√¥ng ƒë∆∞·ª£c t·∫£i).")
            # Gi·ªØ lu·ªìng ch√≠nh ch·∫°y
            while not main_detector.stop_event.is_set():
                try:
                    time.sleep(1)
                except KeyboardInterrupt:
                    print("\nPh√°t hi·ªán Ctrl+C! ƒêang d·ª´ng detector...")
                    break


    except RuntimeError as re: # B·∫Øt c·ª• th·ªÉ l·ªói Runtime t·ª´ init c·ªßa EmotionDetector
        print(f"L·ªói RuntimeError khi kh·ªüi t·∫°o EmotionDetector: {re}")
    except Exception as e:
        print(f"ƒê√£ x·∫£y ra l·ªói kh√¥ng mong mu·ªën trong __main__: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("B·∫Øt ƒë·∫ßu qu√° tr√¨nh d·ªçn d·∫πp cu·ªëi c√πng trong __main__...")
        if main_detector:
            print("Y√™u c·∫ßu EmotionDetector d·ª´ng c√°c lu·ªìng v√† d·ªçn d·∫πp...")
            main_detector.stop() # stop() s·∫Ω g·ªçi cleanup() b√™n trong n√≥
            
            # L∆∞u l·ªãch s·ª≠ c·∫£m x√∫c (n·∫øu c·∫ßn thi·∫øt v√† logic v·∫´n gi·ªØ)
            if hasattr(main_detector, 'emotion_history') and main_detector.emotion_history:
                 from db_utils import save_all_emotions_to_db
                 save_all_emotions_to_db("emotion_log.db", main_detector.emotion_history)
                 print("‚úÖ ƒê√£ l∆∞u to√†n b·ªô l·ªãch s·ª≠ c·∫£m x√∫c v√†o c∆° s·ªü d·ªØ li·ªáu.")
            else:
                print("Kh√¥ng c√≥ l·ªãch s·ª≠ c·∫£m x√∫c ƒë·ªÉ l∆∞u ho·∫∑c thu·ªôc t√≠nh kh√¥ng t·ªìn t·∫°i.")
        
        # if EmotionGUI and root and hasattr(root, 'destroy'): # ƒê√≥ng c·ª≠a s·ªï giao di·ªán n·∫øu c√≥
        #     print("ƒêang ƒë√≥ng c·ª≠a s·ªï giao di·ªán (n·∫øu c√≥)...")
        #     # root.destroy() # mainloop() s·∫Ω t·ª± x·ª≠ l√Ω vi·ªác n√†y khi tho√°t

        print("Ch∆∞∆°ng tr√¨nh ƒë√£ k·∫øt th√∫c ho√†n to√†n.")