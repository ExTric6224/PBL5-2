# emotion_detector.py
import cv2
import numpy as np
import argparse
import os
import time
import threading
from datetime import datetime
import socket
import struct
# import queue # C√≥ th·ªÉ v·∫´n c·∫ßn sau n√†y cho giao ti·∫øp ph·ª©c t·∫°p h∆°n

# --- Nh·∫≠p c√°c l·ªõp Analyzer ---
from face_analyzer import FaceAnalyzer
from emotion_history_item import EmotionHistoryItem
from voice_analyzer import VoiceAnalyzer

# db
from db_utils import load_emotion_history_from_db

# --- Th∆∞ vi·ªán √¢m thanh (v·∫´n gi·ªØ) ---
try:
    # import sounddevice as sd
    SOUND_DEVICE_AVAILABLE = True
except ImportError:
    SOUND_DEVICE_AVAILABLE = False
except Exception:
    SOUND_DEVICE_AVAILABLE = False

class EmotionDetector:
    """
    Qu·∫£n l√Ω c√°c lu·ªìng x·ª≠ l√Ω n·ªÅn (khu√¥n m·∫∑t, gi·ªçng n√≥i) v√† cung c·∫•p
    d·ªØ li·ªáu (frame, c·∫£m x√∫c) cho b√™n ngo√†i (v√≠ d·ª•: UIController th√¥ng qua lu·ªìng ch√≠nh).
    """
    def __init__(self, face_analyzer,voice_analyzer,cascade_path='src/haarcascade_frontalface_default.xml',
             enable_analysis_face=True, enable_analysis_voice=True):
        # --- Components ---
        if not isinstance(face_analyzer, FaceAnalyzer):
             raise TypeError("'face_analyzer' ph·∫£i l√† m·ªôt instance c·ªßa FaceAnalyzer.")
        self.face_analyzer = face_analyzer
        self.voice_analyzer = voice_analyzer  # Initialize as an instance of VoiceAnalyzer
        self.face_cascade = self._load_cascade(cascade_path)

        if self.face_cascade is None:
            raise RuntimeError("Kh√¥ng th·ªÉ t·∫£i Haar cascade.")

        # --- Shared Data ---
        self.latest_frame = None
        self.last_face_emotion = "N/A"
        self.last_face_emotion_probabilities = None # C√≥ th·ªÉ d√πng sau n√†y
        
        self.last_voice_emotion = "N/A (Placeholder)"
        self.last_voice_probabilities = None  # L∆∞u probabilities ƒë·ªÉ g·ª≠i cho UI
        self.frame_lock = threading.Lock()
        self.emotion_lock = threading.Lock()

        # --- Thread Control ---
        self.stop_event = threading.Event()
        self.face_thread = None
        self.voice_thread = None
        self.cap = None
        
        self.enable_analysis_face = enable_analysis_face
        self.enable_analysis_voice = enable_analysis_voice
        self.emotion_history = []  # Danh s√°ch l∆∞u tr·ªØ c√°c EmotionHistoryItem
        self.emotion_history = load_emotion_history_from_db("emotion_log.db")
        self.can_send_to_UI = True;
        
        ##socket
        self.socket = None
        self.connection = None


    def _load_cascade(self, cascade_path):
        # (Gi·ªØ nguy√™n)  
        if not os.path.exists(cascade_path): return None
        try:
            cascade = cv2.CascadeClassifier(cascade_path)
            if cascade.empty(): raise IOError("Cascade tr·ªëng")
            return cascade
        except Exception as e: print(f"L·ªói t·∫£i cascade: {e}"); return None

    def _init_webcam(self):
        
        try:
            self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.socket.bind(('0.0.0.0', 9999))  # L·∫Øng nghe tr√™n t·∫•t c·∫£ IP m√°y
            self.socket.listen(1)
            print("üïì ƒêang ch·ªù Raspberry Pi k·∫øt n·ªëi...")
            self.conn, _ = self.socket.accept()
            self.connection = self.conn.makefile('rb')
            print("‚úÖ Raspberry Pi ƒë√£ k·∫øt n·ªëi.")
            return True
        except Exception as e:
            print(f"L·ªói socket: {e}")
            return False


    # --- Lu·ªìng x·ª≠ l√Ω khu√¥n m·∫∑t ---
    def _face_processing_loop(self):
        print("Lu·ªìng webcam: B·∫Øt ƒë·∫ßu.")
        if not self.cap:
            print("L·ªói: Webcam ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o.")
            return

        while not self.stop_event.is_set():
            if self.can_send_to_UI:
                image_len_data = self.connection.read(4)
                if not image_len_data:
                    time.sleep(0.1)
                    continue

                image_len = struct.unpack('>L', image_len_data)[0]
                image_data = self.connection.read(image_len)

                image_array = np.frombuffer(image_data, dtype=np.uint8)
                frame = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
                ret = frame is not None

                if not ret:
                    print("L·ªói ƒë·ªçc frame t·ª´ webcam.")
                    time.sleep(0.5)
                    continue

                if self.enable_analysis_face:   
                    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                    faces = self.face_cascade.detectMultiScale(gray_frame, 1.1, 5, minSize=(40, 40))
                    processed_frame = frame.copy()
                    current_face_emotion_in_frame = "N/A"

                    for (x, y, w, h) in faces:
                        cv2.rectangle(processed_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
                        roi_gray = gray_frame[y:y + h, x:x + w]
                        predicted = self.face_analyzer.analyzeFace(roi_gray)
                        if predicted:
                            # L·∫•y nh√£n c·∫£m x√∫c c√≥ x√°c su·∫•t cao nh·∫•t
                            current_face_emotion_in_frame, prob = max(predicted.items(), key=lambda item: item[1])
                            break

                    with self.frame_lock:
                        self.latest_frame = processed_frame

                    if current_face_emotion_in_frame != "N/A":
                        with self.emotion_lock:
                            if self.last_face_emotion != current_face_emotion_in_frame:
                                # Th√™m v√†o danh s√°ch emotion_history
                                emotion_item = EmotionHistoryItem(
                                    timestamp=datetime.now(),
                                    face_location=f"{x}x{y}",
                                    duration=None,  # Ho·∫∑c duration t·∫°m th·ªùi n·∫øu b·∫°n ƒëo ƒë∆∞·ª£c th·ªùi gian hi·ªán di·ªán
                                    result=current_face_emotion_in_frame,
                                    source="Webcam",
                                    emotion_distribution=predicted
                                )
                                self.emotion_history.append(emotion_item)
    
                            self.last_face_emotion = current_face_emotion_in_frame
                            self.last_face_emotion_probabilities = prob

                else:
                    # Kh√¥ng ph√¢n t√≠ch: ch·ªâ hi·ªÉn th·ªã frame g·ªëc
                    with self.frame_lock:
                        self.latest_frame = frame.copy()
                    with self.emotion_lock:
                        self.last_face_emotion = "N/A"
                        self.last_face_emotion_probabilities = 0.0
            else:
                # N·∫øu kh√¥ng g·ª≠i ƒë∆∞·ª£c ƒë·∫øn UI, ch·ªâ c·∫ßn ch·ªù m·ªôt ch√∫t
                time.sleep(2)
        print("Lu·ªìng webcam: ƒê√£ d·ª´ng.")
        if self.cap:
            self.cap.release()
            print("Webcam ƒë√£ ƒë∆∞·ª£c gi·∫£i ph√≥ng.")

    def get_latest_data(self):
        """L·∫•y d·ªØ li·ªáu m·ªõi nh·∫•t t·ª´ c√°c lu·ªìng (thread-safe)."""
        frame = None
        face_emo = "N/A"
        face_emo_prob = 0.0
        voice_emo = "N/A (Placeholder)"
        voice_emo_pros = None

        with self.frame_lock:
            if self.latest_frame is not None:
                frame = self.latest_frame.copy() # Tr·∫£ v·ªÅ b·∫£n sao

        with self.emotion_lock:
            face_emo = self.last_face_emotion
            face_emo_prob = self.last_face_emotion_probabilities if self.last_face_emotion_probabilities else 0.0
            voice_emo = self.last_voice_emotion
            voice_emo_pros = self.last_voice_probabilities if self.last_voice_probabilities else None

        return frame, face_emo,face_emo_prob , voice_emo, voice_emo_pros

    # --- Lu·ªìng x·ª≠ l√Ω gi·ªçng n√≥i (Placeholder) ---
    def _voice_processing_loop(self):
        """V√≤ng l·∫∑p x·ª≠ l√Ω √¢m thanh b·∫±ng VoiceAnalyzer m·ªõi."""
        print("Lu·ªìng √¢m thanh: B·∫Øt ƒë·∫ßu.")
        while not self.stop_event.is_set():
            if not self.enable_analysis_voice:
                probabilities = {label: 0.0 for label in self.voice_analyzer.emotion_labels}
                with self.emotion_lock:
                    self.last_voice_emotion = "N/A"
                    self.last_voice_probabilities = probabilities
                time.sleep(1)
                continue

            try:
                # Ghi √¢m v√† nh·∫≠n m·∫£ng √¢m thanh
                audio_array = self.voice_analyzer.record_audio()

                # D·ª± ƒëo√°n c·∫£m x√∫c
                probabilities = self.voice_analyzer.predict_emotion(audio_array)

                if "error" in probabilities:
                    print("L·ªói khi tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng. B·ªè qua l·∫ßn n√†y.")
                    continue

                with self.emotion_lock:
                    self.last_voice_emotion = max(probabilities, key=probabilities.get)
                    self.last_voice_probabilities = probabilities

                    # Ghi l·ªãch s·ª≠ v√†o RAM (ho·∫∑c DB n·∫øu b·∫°n mu·ªën)
                    emotion_item = EmotionHistoryItem(
                        timestamp=datetime.now(),
                        face_location=None,
                        duration=int(self.voice_analyzer.duration_sec * 1000),
                        result=self.last_voice_emotion,
                        source="Microphone",
                        emotion_distribution=probabilities
                    )
                    self.emotion_history.append(emotion_item)

            except Exception as e:
                print(f"L·ªói trong lu·ªìng √¢m thanh: {e}")

            time.sleep(1)
        print("Lu·ªìng √¢m thanh: ƒê√£ d·ª´ng.")


    # --- ƒêi·ªÅu khi·ªÉn ch√≠nh ---
    def start(self):
        """Kh·ªüi t·∫°o webcam v√† b·∫Øt ƒë·∫ßu c√°c lu·ªìng x·ª≠ l√Ω."""
        if self.face_thread or self.voice_thread: return # ƒê√£ ch·∫°y

        self.cap = self._init_webcam()
        if not self.cap: raise RuntimeError("Kh√¥ng th·ªÉ kh·ªüi t·∫°o webcam.")

        self.stop_event.clear()
        print("B·∫Øt ƒë·∫ßu c√°c lu·ªìng x·ª≠ l√Ω n·ªÅn...")
        self.face_thread = threading.Thread(target=self._face_processing_loop, daemon=True)
        self.voice_thread = threading.Thread(target=self._voice_processing_loop, daemon=True)
        self.face_thread.start()
        self.voice_thread.start()

    def stop(self):
        """B√°o hi·ªáu d·ª´ng cho c√°c lu·ªìng v√† ƒë·ª£i ch√∫ng k·∫øt th√∫c."""
        if self.stop_event.is_set(): return # ƒê√£ y√™u c·∫ßu d·ª´ng
        print("B·∫Øt ƒë·∫ßu qu√° tr√¨nh d·ª´ng EmotionDetector...")
        self.stop_event.set()

        threads_to_join = [self.face_thread, self.voice_thread]
        for t in threads_to_join:
             if t and t.is_alive():
                 print(f"ƒêang ƒë·ª£i lu·ªìng {t.name} d·ª´ng...")
                 t.join(timeout=2.0)
                 if t.is_alive(): print(f"C·∫£nh b√°o: Lu·ªìng {t.name} kh√¥ng d·ª´ng k·ªãp th·ªùi.")

        print("C√°c lu·ªìng x·ª≠ l√Ω n·ªÅn ƒë√£ d·ª´ng.")
        # Kh√¥ng g·ªçi cleanup() ·ªü ƒë√¢y n·ªØa, cleanup ch·ªâ ch·ª©a logic kh√¥ng li√™n quan ƒë·∫øn thread join

    def cleanup(self):
        """D·ªçn d·∫πp t√†i nguy√™n kh√¥ng ƒë∆∞·ª£c qu·∫£n l√Ω b·ªüi lu·ªìng con."""
        print("EmotionDetector: Th·ª±c hi·ªán cleanup cu·ªëi c√πng (n·∫øu c√≥).")
        # V√≠ d·ª•: ƒë√≥ng file log, ng·∫Øt k·∫øt n·ªëi DB,...
        # Webcam v√† audio (sau n√†y) ƒë∆∞·ª£c release trong lu·ªìng c·ªßa ch√∫ng.
        # C·ª≠a s·ªï OpenCV do UIController qu·∫£n l√Ω.
        pass


# --- ƒêi·ªÉm kh·ªüi ch·∫°y ch√≠nh ---
if __name__ == "__main__":
    # --- Import UIController ·ªü ƒë√¢y ---
    from ui_controller import EmotionGUI

    ap = argparse.ArgumentParser(description="Ch·∫°y nh·∫≠n di·ªán c·∫£m x√∫c ƒëa lu·ªìng v·ªõi UI Controller.")
    ap.add_argument("--face_model", default="emotion_model.h5", help="ƒê∆∞·ªùng d·∫´n face model")
    ap.add_argument("--cascade", default="src/haarcascade_frontalface_default.xml", help="ƒê∆∞·ªùng d·∫´n Haar cascade")
    args = ap.parse_args()

    # --- Ki·ªÉm tra file ---
    if not os.path.exists(args.cascade): exit(f"L·ªói: Cascade kh√¥ng t√¨m th·∫•y: {args.cascade}")
    if not os.path.exists(args.face_model): exit(f"L·ªói: Face model kh√¥ng t√¨m th·∫•y: {args.face_model}")

    # --- Kh·ªüi t·∫°o ---
    main_detector = None
    ui_controller = None
    keep_running = True # C·ªù ƒë·ªÉ ƒëi·ªÅu khi·ªÉn v√≤ng l·∫∑p ch√≠nh

    try:
        # 1. Kh·ªüi t·∫°o FaceAnalyzer
        face_analyzer_inst = FaceAnalyzer(model_path=args.face_model)
        
        # kh·ªüi t·∫°o VoiceAnalyzer 
        voi_analyzer_inst = VoiceAnalyzer();
        
        # 2. Kh·ªüi t·∫°o EmotionDetector
        main_detector = EmotionDetector(
            face_analyzer=face_analyzer_inst,
            voice_analyzer=voi_analyzer_inst,
            cascade_path=args.cascade
        )

        # 3. Kh·ªüi t·∫°o giao di·ªán EmotionGUI
        from ttkbootstrap import Style
        style = Style("superhero")  # ho·∫∑c "litera", "flatly", "darkly"
        root = style.master
        gui = EmotionGUI(root)
        gui.detector = main_detector  # G√ÅN emotion detector cho GUI ƒë·ªÉ c√≥ th·ªÉ ƒëi·ªÅu khi·ªÉn enable_analysis
        # 4. B·∫Øt ƒë·∫ßu c√°c lu·ªìng x·ª≠ l√Ω n·ªÅn c·ªßa detector
        main_detector.start()
        print("C√°c lu·ªìng n·ªÅn ƒë√£ b·∫Øt ƒë·∫ßu. B·∫Øt ƒë·∫ßu v√≤ng l·∫∑p hi·ªÉn th·ªã ch√≠nh...")
        time.sleep(1.0) # ƒê·ª£i ch√∫t

        # 5. V√≤ng l·∫∑p hi·ªÉn th·ªã v√† x·ª≠ l√Ω input (trong lu·ªìng ch√≠nh)
        def update_gui():
            frame, face_emo,face_emo_pro, voice_emo, voice_emo_pros = main_detector.get_latest_data()
            gui.update_video_frame(
                frame,
                face_emotion=face_emo,
                emotion_probability=face_emo_pro,
                voice_emotion=voice_emo,
                voice_probabilities=voice_emo_pros
            )

            root.after(30, update_gui)

        update_gui()
        root.mainloop()

    except (ValueError, TypeError, RuntimeError, IOError) as e:
         print(f"L·ªói nghi√™m tr·ªçng khi kh·ªüi t·∫°o ho·∫∑c ch·∫°y: {e}")
         keep_running = False # D·ª´ng v√≤ng l·∫∑p n·∫øu c√≥ l·ªói
    except KeyboardInterrupt:
         print("\nPh√°t hi·ªán Ctrl+C!")
         keep_running = False # D·ª´ng v√≤ng l·∫∑p
    except Exception as e:
        print(f"\nƒê√£ x·∫£y ra l·ªói kh√¥ng mong mu·ªën: {e}")
        import traceback
        traceback.print_exc()
        keep_running = False # D·ª´ng v√≤ng l·∫∑p
    finally:
        print("B·∫Øt ƒë·∫ßu qu√° tr√¨nh d·ªçn d·∫πp cu·ªëi c√πng...")
        if main_detector:
            from db_utils import save_all_emotions_to_db
            save_all_emotions_to_db("emotion_log.db", main_detector.emotion_history)
            print("‚úÖ ƒê√£ l∆∞u to√†n b·ªô l·ªãch s·ª≠ c·∫£m x√∫c v√†o c∆° s·ªü d·ªØ li·ªáu.")
            print("Y√™u c·∫ßu EmotionDetector d·ª´ng c√°c lu·ªìng...")
            main_detector.stop() # ƒê·ª£i c√°c lu·ªìng k·∫øt th√∫c
            main_detector.cleanup() # Ch·∫°y cleanup c·ªßa detector (n·∫øu c√≥)

        # 2. ƒê√≥ng c·ª≠a s·ªï giao di·ªán
        if ui_controller:
            print("Y√™u c·∫ßu UIController ƒë√≥ng c·ª≠a s·ªï...")
            ui_controller.destroy_windows()

        print("Ch∆∞∆°ng tr√¨nh ƒë√£ k·∫øt th√∫c ho√†n to√†n.")
